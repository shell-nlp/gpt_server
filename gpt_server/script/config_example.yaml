# 后台启动 nohup sh start.sh > gptserver.log &
# openai_api_server
serve_args:
  # openai 服务的 host 和 port
  enable: true
  host: 0.0.0.0
  port: 8082
  controller_address: http://localhost:21001 # 控制器的ip地址
  # api_keys: 111,222  # 用来设置 openai 密钥


controller_args:
  # 控制器的配置参数
  enable: true
  host: 0.0.0.0
  port: 21001
  dispatch_method: shortest_queue # lottery、shortest_queue # 现有两种请求分发策略，随机（lottery） 和 最短队列（shortest_queue），最短队列方法更推荐。

model_worker_args:
  # 模型的配置参数，这里port 不能设置，程序自动分配，并注册到 控制器中。
  # model worker 的配置参数
  host: 0.0.0.0
  controller_address: http://localhost:21001 # # 将模型注册到 控制器的 地址
  log_level: WARNING # DEBUG INFO WARNING ERROR
  limit_worker_concurrency: 1024 # worker的最大并发数,默认为 1024

models:
# --------------- 支持的大语言模型样例 ---------------
- qwen:
    # 大语言模型
    #自定义的模型名称
    alias: gpt-4,gpt-3.5-turbo,gpt-3.5-turbo-16k # 别名     例如  gpt4,gpt3
    enable: false # false true
    model_config:
      # 模型的配置参数
      model_name_or_path: /home/dev/model/qwen/Qwen2___5-7B-Instruct/ # 模型的路径
      enable_prefix_caching: true # 是否启用前缀缓存
      dtype: auto # 类型
      max_model_len: 65536 # 模型最大token  长度
      gpu_memory_utilization: 0.8
      kv_cache_quant_policy: 0
      # lora:  # lora 模型的路径
      #   test_lora: /home/dev/project/LLaMA-Factory/saves/Qwen1.5-14B-Chat/lora/train_2024-03-22-09-01-32/checkpoint-100

    model_type: qwen # qwen  yi internlm 等,也可设置为 auto, 现在只有 大语言模型 和 多模态语言模型 支持 auto
    work_mode: lmdeploy-turbomind # vllm/hf/lmdeploy-turbomind/lmdeploy-pytorch

    device: gpu # gpu / cpu
    workers:
    - gpus:
      - 1
      # - gpus:
      #   - 3
      # - gpus:
      #   - 0

      # - gpus:  表示 模型使用 gpu[0,1]，默认使用的 TP(张量并行)
      #   - 0
      #   - 1

      # - gpus:  表示启动两个模型，模型副本1加载到 0卡， 模型副本2 加载到 1卡
      #   - 0
      # - gpus:
      #   - 1


# --------------- 支持的多模态模型样例 ---------------
- internvl2:
    # 多模态模型
    #自定义的模型名称
    alias: null # 别名  例如  gpt4,gpt3
    enable: false # false true  控制是否启动模型worker
    model_config:
      # 模型的配置参数
      model_name_or_path: /home/dev/model/OpenGVLab/InternVL2-40B-AWQ/
      enable_prefix_caching: false
    model_type: internvl2 # qwen  yi internlm ,也可设置为 auto, 现在只有 大语言模型 和 多模态语言模型 支持 auto
    work_mode: lmdeploy-turbomind # vllm/hf/lmdeploy-turbomind/lmdeploy-pytorch
    device: gpu # gpu / cpu
    workers:
    - gpus:
      # - 1
      - 0
# --------------- 支持的rerank模型样例 ---------------
- bge-reranker-base:
    # rerank模型
    alias: null # 别名   
    enable: true # false true
    model_config:
      model_name_or_path: /home/dev/model/Xorbits/bge-reranker-base/
    model_type: embedding
    work_mode: infinity # 可选 ["vllm", "infinity", "sentence_transformers"]，但并不是所有后端都支持
    device: gpu # gpu / cpu
    workers:
    - gpus:
      - 2

- qwen3-reranker:
    alias: null
    enable: true
    model_config:
      model_name_or_path: /home/dev/model/Qwen/Qwen3-Reranker-0___6B/
      dtype: auto
      task_type: reranker
      hf_overrides: { "architectures": [ "Qwen3ForSequenceClassification" ], "classifier_from_token": [ "no", "yes" ], "is_original_qwen3_reranker": True }
    model_type: embedding
    work_mode: vllm
    device: gpu
    workers:
    - gpus:
      - 6
# --------------- 支持的多模态多语言的重排模型样例 ---------------
- jina-reranker:
    # 多模态多语言的重排模型，这个模型task_type 只能是 auto
    alias: null
    enable: true
    model_config:
      model_name_or_path: /home/dev/model/jinaai/jina-reranker-m0/
      task_type: auto # auto 、embedding 、 reranker 或者 classify 不设置这个参数，默认为 auto,自动识别可能会识别错误
    model_type: embedding
    work_mode: sentence_transformers # 可选 ["vllm", "infinity", "sentence_transformers"]，但并不是所有后端都支持
    device: gpu
    workers:
    - gpus:
      - 5
# --------------- 支持的文本embedding模型样例 ---------------
- acge_text_embedding:
    alias: text-embedding-ada-002 # 别名   
    enable: true # false true
    model_config:
      model_name_or_path: /home/dev/model/aspire/acge_text_embedding
      task_type: auto # auto 、embedding 、 reranker 或者 classify 不设置这个参数，默认为 auto,自动识别可能会识别错误
    model_type: embedding
    work_mode: infinity # 可选 ["vllm", "infinity", "sentence_transformers"]，但并不是所有后端都支持
    device: gpu # gpu / cpu
    workers:
    - gpus:
      - 2
# --------------- 支持的vl-embedding 模型样例 --------------- 
- bge-vl:
    alias: null
    enable: true
    model_config:
      model_name_or_path: /home/dev/model/BAAI/BGE-VL-base/
    model_type: embedding
    work_mode: sentence_transformers # 可选 ["vllm", "infinity", "sentence_transformers"]，但并不是所有后端都支持
    device: gpu
    workers:
    - gpus:
      - 2
# --------------- 支持的文本审核模型样例 --------------- 
- text-moderation:
    alias: omni-moderation-latest
    enable: true
    model_config:
      model_name_or_path: /home/dev/model/KoalaAI/Text-Moderation
    model_type: embedding
    work_mode: infinity # 可选 ["vllm", "infinity", "sentence_transformers"]，但并不是所有后端都支持
    device: gpu
    workers:
    - gpus:
      - 2
# --------------- 支持的最新支持ASR模型样例 --------------- 
- SenseVoiceSmall:
    alias: null
    enable: true
    model_config:
      model_name_or_path: /home/dev/model/iic/SenseVoiceSmall # 模型路径
      vad_model: /home/dev/model/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch/ # VAD模型，可以不设置
    model_type: funasr # 类型只能是 funasr
    work_mode: hf
    device: gpu
    workers:
    - gpus:
      - 2
# --------------- 支持的TTS 模型的配置方式样例 --------------- 
- tts:
    alias: null
    enable: true
    model_config:
      model_name_or_path: /home/dev/model/SparkAudio/Spark-TTS-0___5B/
    model_type: spark_tts
    work_mode: vllm
    device: gpu
    workers:
    - gpus:
      - 6
# --------------- 支持的文生图模型样例 --------------- 
- flux:

    alias: null
    enable: true
    model_config:
      model_name_or_path: /home/dev/model/MusePublic/489_ckpt_FLUX_1/
    model_type: flux
    work_mode: hf
    device: gpu
    workers:
    - gpus:
      - 7

- qwen-image:
    alias: null
    enable: true
    model_config:
      model_name_or_path: /home/dev/model/Qwen/Qwen-Image/
    model_type: qwen_image
    work_mode: hf
    device: gpu
    workers:
    - gpus:
      - 7
# --------------- 支持的图片编辑模型样例 --------------- 
- image-edit:
    alias: null
    enable: true
    model_config:
      model_name_or_path: /home/dev/model/Qwen/Qwen-Image-Edit/
    model_type: qwen_image_edit
    work_mode: hf
    device: gpu
    port: 8084 # 支持手动设置端口
    workers:
    - gpus:
      - 7
